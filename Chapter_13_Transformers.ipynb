{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Transformers\n",
        "\n",
        "Bab ini mengeksplorasi arsitektur Transformer yang merevolusi dunia NLP. Tidak seperti RNN yang memproses kata secara berurutan, Transformer memproses seluruh kalimat sekaligus menggunakan mekanisme *Attention*.\n",
        "\n",
        "---\n",
        "\n",
        "## 13.1 Komponen Dasar Transformer\n",
        "Arsitektur ini tetap mempertahankan struktur **Encoder-Decoder**:\n",
        "* **Encoder**: Terdiri dari *Self-Attention layer* dan *Fully Connected layer*. Fungsinya memahami konteks seluruh kalimat input.\n",
        "* **Decoder**: Terdiri dari *Masked Self-Attention*, *Encoder-Decoder Attention*, dan *Fully Connected layer*. Fungsinya menghasilkan output berdasarkan konteks dari Encoder.\n",
        "* **Q, K, V**: Mekanisme *Self-Attention* didasarkan pada matriks bobot **Query** (apa yang dicari), **Key** (relevansi informasi), dan **Value** (isi informasi).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 13.2 Embeddings: Token & Positional\n",
        "Karena Transformer memproses semua kata secara paralel, ia kehilangan informasi urutan kata. Untuk mengatasinya, digunakan dua jenis embedding:\n",
        "1.  **Token Embeddings**: Representasi angka unik untuk setiap kata berdasarkan makna semantiknya.\n",
        "2.  **Positional Embeddings**: Informasi tambahan yang disuntikkan ke dalam model untuk memberi tahu posisi setiap kata (misal: kata \"Saya\" berada di posisi ke-1).\n",
        "* **Unsupervised Learning**: Model dilatih untuk memprediksi kata yang hilang sehingga membentuk matriks embedding yang cerdas.\n",
        "\n",
        "---\n",
        "\n",
        "## 13.3 Residual Connections & Layer Normalization\n",
        "Untuk melatih jaringan yang sangat dalam tanpa mengalami *Vanishing Gradients*, Transformer menggunakan:\n",
        "* **Residual Connections (Shortcut)**: Menambahkan input asli langsung ke output layer. Ini menciptakan jalur cepat bagi gradien untuk mengalir selama *backpropagation*.\n",
        "* **Layer Normalization**: Menghitung rata-rata dan varians dari aktivasi untuk menstabilkan pelatihan dan mengurangi masalah *Internal Covariate Shift*.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 13.4 BERT (Bidirectional Encoder Representations from Transformers)\n",
        "BERT adalah salah satu model Transformer paling populer yang hanya menggunakan bagian **Encoder**.\n",
        "* **Pretraining**: Dilatih secara *unsupervised* dengan dua tugas:\n",
        "    1.  **MLM (Masked Language Modeling)**: Menebak kata yang disembunyikan di tengah kalimat.\n",
        "    2.  **NSP (Next Sentence Prediction)**: Menebak apakah dua kalimat berhubungan.\n",
        "* **Token Spesial**:\n",
        "    * `[CLS]`: Token di awal kalimat yang digunakan sebagai representasi untuk klasifikasi.\n",
        "    * `[SEP]`: Token pemisah antara dua kalimat yang berbeda.\n",
        "\n",
        "---\n",
        "\n",
        "## 13.5 Klasifikasi Spam & Penanganan Imbalance\n",
        "Dalam kasus nyata seperti deteksi spam, data seringkali tidak seimbang (lebih banyak pesan normal daripada spam).\n",
        "* **Library imbalanced-learn**: Digunakan untuk menyeimbangkan dataset melalui:\n",
        "    * **Undersampling**: Mengurangi jumlah data kelas mayoritas.\n",
        "    * **Oversampling**: Menambah jumlah data kelas minoritas.\n",
        "* **Classification Head**: Lapisan tambahan (Dense layer) yang diletakkan di atas output BERT untuk menghasilkan prediksi akhir (Spam vs Bukan).\n",
        "\n",
        "---\n",
        "\n",
        "## 13.6 Hugging Face & Question Answering\n",
        "Library **Hugging Face** menyediakan akses mudah ke model *pretrained* seperti **DistilBERT** (versi BERT yang lebih ringan).\n",
        "* **SQuAD Dataset**: Standar dataset untuk tugas *Question Answering*.\n",
        "* **Mekanisme**: Model menerima dua input (Pertanyaan + Paragraf konteks) dan harus memprediksi posisi awal dan akhir jawaban di dalam paragraf tersebut.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 13.7 Integritas Data & Pipeline\n",
        "Masalah umum dalam NLP adalah penyelarasan (*alignment*) antara teks asli dan token ID.\n",
        "* **Tokenizer**: Menangani pemecahan kata, penambahan token spesial, dan *alignment*.\n",
        "* **Padding & Truncating**: Kalimat pendek diberi token `[PAD]`, kalimat panjang dipotong.\n",
        "* **tf.data.Dataset**: Membuat pipeline efisien yang mengelola *batching* untuk data training dan validasi.\n",
        "\n",
        "---\n",
        "\n",
        "## 13.8 Tip Instalasi di Colab\n",
        "Untuk menggunakan model-model di bab ini, kamu wajib menginstal library Transformers:\n",
        "\n",
        "```python\n",
        "# Instalasi library Hugging Face\n",
        "!pip install transformers datasets\n",
        "\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Memanggil model BERT yang sudah dilatih (pretrained)\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "rh-drGB8_k1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9DuXP36_gsc"
      },
      "outputs": [],
      "source": []
    }
  ]
}