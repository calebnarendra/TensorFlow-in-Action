{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Improving CNNs and Making Them Confess\n",
        "\n",
        "Bab ini berfokus pada teknik lanjutan untuk meningkatkan performa model (mencegah overfitting) serta teknik interpretabilitas model agar kita memahami alasan di balik prediksi sebuah AI.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.1 Image Data Augmentation\n",
        "Augmentasi data adalah teknik kunci untuk mengurangi **overfitting** dengan cara memperbanyak variasi data tanpa harus mengambil gambar baru.\n",
        "* **Metode**: Mengubah kontras (*contrast*), kecerahan (*brightness*), melakukan zoom, rotasi, pergeseran (*width/height shift*), hingga membalik gambar (*horizontal flip*).\n",
        "* **Implementasi**: Sangat mudah dilakukan melalui `tf.keras.preprocessing.image.ImageDataGenerator` yang melakukan transformasi secara acak dan *on-the-fly* selama proses training.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.2 Teknik Regularisasi: Dropout\n",
        "Dropout membantu menciptakan jaringan yang lebih tangguh (*well-behaved*) dengan mencegah ketergantungan antar neuron.\n",
        "* **Cara Kerja**: Mematikan (mengabaikan) neuron secara acak pada setiap iterasi selama fase *training*.\n",
        "* **Manfaat**: Memaksa setiap neuron untuk mempelajari fitur yang berguna secara mandiri, sehingga model lebih tergeneralisasi dan tidak hanya menghafal data latihan.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.3 Early Stopping\n",
        "Metode optimasi yang menghentikan proses pelatihan secara otomatis sebelum jumlah *epoch* yang ditentukan selesai.\n",
        "* **Logika**: Jika akurasi pada data validasi berhenti meningkat atau justru menurun (indikasi mulai overfitting), pelatihan akan dihentikan untuk mengambil versi model terbaik sebelumnya.\n",
        "* **Keras Callback**: Diimplementasikan melalui `tf.keras.callbacks.EarlyStopping`.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.4 Arsitektur Minception (Inception-ResNet)\n",
        "Minception adalah evolusi dari Inception yang mengadopsi konsep **Residual Learning** untuk mempermudah pelatihan jaringan yang sangat dalam.\n",
        "\n",
        "### 1. Implementasi Stem\n",
        "Berbeda dengan model tradisional, Stem pada Minception menggunakan gabungan **BatchNorm + ReLU** setelah setiap operasi konvolusi.\n",
        "* **Batch Normalization (BN)**: Menstabilkan proses latihan dengan menormalisasi input setiap lapisan, memungkinkan penggunaan *learning rate* yang lebih tinggi.\n",
        "\n",
        "### 2. Inception-ResNet Blocks\n",
        "* **Type A Block**: Menggunakan koneksi residual (penjumlahan input ke output) setelah penggabungan konvolusi paralel. Ini membantu gradien mengalir lebih baik.\n",
        "* **Type B Block**: Versi yang lebih sederhana dengan dua jalur paralel, dirancang untuk efisiensi komputasi tanpa kehilangan kekuatan fitur.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7.5 Strategi Pelatihan: Learning Rate Scheduling\n",
        "Selain arsitektur, jadwal penurunan *learning rate* sangat menentukan hasil akhir.\n",
        "* **Konsep**: Saat model mulai mendekati nilai optimal, *learning rate* diperkecil secara bertahap agar model tidak melompati titik minimum global.\n",
        "* **Hasil**: Meningkatkan generalisasi dan stabilitas model pada tahap akhir pelatihan.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.6 Transfer Learning\n",
        "Teknik \"berdiri di atas bahu raksasa\". Kita menggunakan model yang sudah dilatih pada dataset masif (seperti ImageNet) dan mengadaptasinya untuk tugas kita.\n",
        "* **Proses**: Membekukan (*freeze*) lapisan awal model yang sudah pintar mengenali fitur umum, lalu melatih ulang lapisan akhir (*top layers*) untuk mengenali kelas spesifik kita.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.7 Grad-CAM: Membuat Model \"Mengaku\"\n",
        "**Grad-CAM (Gradient-weighted Class Activation Mapping)** digunakan untuk memvisualisasikan bagian mana dari gambar yang paling mempengaruhi keputusan model.\n",
        "* **Mekanisme**: Menggunakan magnitudo gradien yang mengalir ke lapisan konvolusi terakhir.\n",
        "* **Output**: Menghasilkan *heatmap* warna pada gambar asli. Bagian merah menunjukkan area yang menjadi alasan utama model memberikan label tertentu (misal: model mendeteksi \"Anjing\" karena melihat area telinga dan hidung).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7.8 Tips Praktis di Colab\n",
        "Gunakan Callback untuk Early Stopping dan Learning Rate:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Berhenti jika val_loss tidak turun selama 5 epoch\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Kurangi learning rate jika val_loss stagnan\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)\n",
        "\n",
        "# Masukkan ke dalam model.fit\n",
        "# model.fit(train_data, callbacks=[early_stop, lr_reduction])"
      ],
      "metadata": {
        "id": "QDCR50LI93gY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52rWJgTL9030"
      },
      "outputs": [],
      "source": []
    }
  ]
}