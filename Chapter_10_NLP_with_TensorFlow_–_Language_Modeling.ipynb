{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10: NLP with TensorFlow â€“ Language Modeling\n",
        "\n",
        "Bab ini membahas dasar-dasar *Language Modeling*, yaitu kemampuan model untuk memprediksi kata berikutnya dalam sebuah urutan teks, serta bagaimana mengukur kualitas teks yang dihasilkan oleh AI.\n",
        "\n",
        "---\n",
        "\n",
        "## 10.1 Konsep Language Modeling\n",
        "*Language Modeling* adalah tugas inti dalam NLP yang bertujuan menghitung probabilitas sebuah kata muncul setelah urutan kata tertentu.\n",
        "* **Tujuan**: Membangun model yang memahami struktur bahasa sehingga dapat menghasilkan kalimat yang koheren.\n",
        "* **Trainable Parameters**: Model belajar dari korpus teks besar untuk menyesuaikan bobot (parameter) agar prediksi kata berikutnya semakin akurat.\n",
        "\n",
        "---\n",
        "\n",
        "## 10.2 Pendekatan N-grams\n",
        "Sebelum era Deep Learning, N-grams adalah metode standar untuk memodelkan bahasa.\n",
        "* **Mekanisme**: Membagi teks menjadi potongan-potongan kecil dengan panjang $N$ (misal: *Bigram* untuk 2 kata, *Trigram* untuk 3 kata).\n",
        "* **Karakteristik**: Bisa dibuat saling tumpang tindih (*overlapping*) untuk menangkap transisi antar kata, namun sulit menangkap hubungan jarak jauh dalam kalimat.\n",
        "\n",
        "---\n",
        "\n",
        "## 10.3 Tokenisasi & Pipeline tf.data\n",
        "Proses pengolahan data untuk *Language Modeling* memerlukan persiapan khusus:\n",
        "* **Tokenizing**: Memecah kalimat menjadi token (kata/karakter) dan memberikan ID unik untuk setiap token.\n",
        "* **Windowing**: Menggunakan `tf.data.Dataset.window()` untuk menciptakan potongan sekuens teks bergeser.\n",
        "    * Contoh: Dari kalimat \"Saya belajar TensorFlow\", model dibuat belajar bahwa \"Saya\" memprediksi \"belajar\", dan \"Saya belajar\" memprediksi \"TensorFlow\".\n",
        "* **Flattening**: Mengubah struktur *windowed data* yang kompleks menjadi dataset satu dimensi yang siap diproses oleh model.\n",
        "\n",
        "---\n",
        "\n",
        "## 10.4 Generating Text dengan GRU (Gated Recurrent Units)\n",
        "GRU adalah variasi dari RNN yang lebih efisien daripada LSTM karena memiliki arsitektur yang lebih sederhana namun tetap mampu menangani masalah memori jangka pendek.\n",
        "* **Update Gate**: Menentukan seberapa banyak informasi dari *hidden state* sebelumnya yang harus disimpan untuk masa depan.\n",
        "* **Reset Gate**: Menentukan seberapa banyak informasi masa lalu yang harus dilupakan.\n",
        "* **Keunggulan**: Memerlukan parameter lebih sedikit dan waktu komputasi yang lebih cepat dibandingkan LSTM, namun dengan performa yang bersaing.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 10.5 Mengukur Kualitas Teks: Perplexity\n",
        "Bagaimana kita tahu model bahasa itu \"pintar\"?\n",
        "* **Entropi**: Mengukur tingkat ketidakpastian atau informasi dalam sebuah urutan kata.\n",
        "* **Perplexity**: Metrik evaluasi utama dalam *Language Modeling*. Perplexity yang rendah menunjukkan bahwa model tidak \"terkejut\" oleh data tes, yang berarti model mampu memprediksi kata-kata dengan percaya diri dan akurat.\n",
        "\n",
        "---\n",
        "\n",
        "## 10.6 Strategi Decoding: Menghasilkan Teks\n",
        "Setelah model dilatih, kita perlu strategi untuk memilih kata saat proses generasi:\n",
        "\n",
        "1.  **Greedy Decoding**: Selalu memilih kata dengan probabilitas tertinggi.\n",
        "    * *Masalah*: Sering menghasilkan teks yang berulang-ulang dan kaku. Diperlukan logika tambahan (seperti *Random Sampling* atau *Temperature*) untuk meningkatkan variasi.\n",
        "2.  **Beam Search**: Daripada hanya satu kata, model menyimpan beberapa kandidat kalimat terbaik (jalur) sekaligus dan mengevaluasi mana yang paling masuk akal beberapa langkah ke depan.\n",
        "    * *Hasil*: Kalimat yang dihasilkan jauh lebih akurat dan natural secara tata bahasa.\n",
        "\n",
        "---\n",
        "\n",
        "## 10.7 Implementasi Tambahan (Instalasi & Tooling)\n",
        "Pada tahap ini, kamu mungkin mulai memerlukan library tambahan untuk menangani teks skala besar:\n",
        "\n",
        "```python\n",
        "# Seringkali diperlukan untuk pemrosesan teks tingkat lanjut\n",
        "!pip install tensorflow-text\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GRU, Embedding, Dense\n",
        "\n",
        "# Contoh sederhana membangun model bahasa\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=64),\n",
        "    GRU(128, return_sequences=True),\n",
        "    Dense(1000, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "e5kWxRvb-xmc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gf-hTbQ-r1U"
      },
      "outputs": [],
      "source": []
    }
  ]
}