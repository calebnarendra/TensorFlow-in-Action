{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 9: NLP with TensorFlow â€“ Sentiment Analysis\n",
        "\n",
        "Bab ini mengeksplorasi bagaimana Natural Language Processing (NLP) digunakan untuk mengklasifikasikan teks berdasarkan sentimen (positif/negatif). Fokus utama adalah pada pembersihan data teks dan penggunaan memori jangka panjang melalui LSTM.\n",
        "\n",
        "---\n",
        "\n",
        "## 9.1 Preprocessing Teks (Pembersihan Noise)\n",
        "Data teks mentah sangat kotor dan tidak terstruktur. Langkah pembersihan meliputi:\n",
        "* **Normalisasi**: Mengecilkan huruf (*lowercasing*) dan menghapus tanda baca unik.\n",
        "* **Stopwords Removal**: Menghapus kata-kata umum yang tidak informatif (seperti \"dan\", \"yang\", \"di\").\n",
        "* **Lemmatization/Stemming**: Mengonversi kata ke bentuk dasarnya (misal: \"berlari\" menjadi \"lari\").\n",
        "* **Masalah Class Imbalance**: Seringkali jumlah ulasan positif jauh lebih banyak daripada negatif. Hal ini harus diseimbangkan agar model tidak bias.\n",
        "\n",
        "---\n",
        "\n",
        "## 9.2 Splitting & Strategi Dataset\n",
        "Untuk memastikan evaluasi yang jujur, dataset dibagi secara ketat:\n",
        "* **Validation & Test Set**: Harus dibuat seimbang (*balanced*) agar metrik akurasi mencerminkan kemampuan model yang sebenarnya.\n",
        "* **Training Set**: Berisi sisa data untuk proses pembelajaran model.\n",
        "\n",
        "---\n",
        "\n",
        "## 9.3 Analisis Vocabulary & Panjang Sekuens\n",
        "Dua hyperparameter kunci dalam NLP adalah ukuran kosakata dan panjang kalimat:\n",
        "* **Vocabulary Analysis**: Menggunakan `collections.Counter` untuk menghitung frekuensi kata. Kita perlu membatasi jumlah kata unik (misal: hanya mengambil 10.000 kata terpopuler) untuk efisiensi memori.\n",
        "* **Sequence Length**: Menggunakan `pd.Series.str.len()` untuk melihat distribusi panjang kalimat. Kalimat yang terlalu panjang akan dipotong, dan yang terlalu pendek akan ditambah nol (*padding*).\n",
        "\n",
        "---\n",
        "\n",
        "## 9.4 Tokenisasi: Mengubah Teks ke Angka\n",
        "Komputer hanya memproses angka, sehingga teks harus diubah menjadi ID numerik.\n",
        "* **Tokenizer Keras**: Memetakan setiap kata unik menjadi integer.\n",
        "* **OOD (Out of Vocabulary)**: Menyiapkan token khusus (seperti `<OOV>`) untuk menangani kata-kata yang tidak ada dalam daftar kosakata saat fase testing.\n",
        "\n",
        "---\n",
        "\n",
        "## 9.5 NLP Pipeline & Bucketing\n",
        "Mengolah teks secara efisien memerlukan teknik khusus pada `tf.data`:\n",
        "* **Bucketing**: Mengelompokkan kalimat dengan panjang yang mirip ke dalam satu *batch*. Ini mencegah pemborosan komputasi akibat terlalu banyak *padding* nol pada kalimat pendek yang dipasangkan dengan kalimat sangat panjang.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9.6 Arsitektur LSTM (Long Short-Term Memory)\n",
        "LSTM adalah jenis RNN yang dirancang untuk mengingat konteks jarak jauh dalam sebuah kalimat.\n",
        "* **Dimensi Input**: Memerlukan format `(Batch, Time Steps, Features)`.\n",
        "* **Mekanisme**: Menggunakan *state vectors* untuk menyimpan memori dari kata-kata sebelumnya.\n",
        "* **Layer Stack**: Biasanya disusun dengan urutan: `Embedding` -> `Masking` (mengabaikan padding) -> `LSTM` -> `Dropout` -> `Dense` (dengan aktivasi Sigmoid untuk output biner).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9.7 Word Embeddings (Injeksi Semantik)\n",
        "Embedding adalah teknik paling modern untuk mewakili kata.\n",
        "* **Vektor Padat**: Setiap kata diwakili oleh vektor angka (misal: 128 dimensi).\n",
        "* **Hubungan Semantik**: Kata dengan makna mirip (seperti \"Raja\" dan \"Pangeran\") akan memiliki posisi vektor yang berdekatan dalam ruang matematis.\n",
        "* **Efisiensi**: Ukuran vektor tetap, sehingga penggunaan memori jauh lebih hemat dibandingkan teknik *One-Hot Encoding* yang sangat boros ruang.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9.8 Tip Implementasi di Colab\n",
        "Untuk NLP, penggunaan `TextVectorization` layer sangat disarankan di TensorFlow 2:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Mendefinisikan layer standarisasi teks\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=10000,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=250)\n",
        "\n",
        "# Adaptasi dengan data teks\n",
        "vectorize_layer.adapt(train_text_data)"
      ],
      "metadata": {
        "id": "EJQArHj2-bYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtBgA4na-WjQ"
      },
      "outputs": [],
      "source": []
    }
  ]
}