{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12: Sequence-to-sequence Learning (Part 2) â€“ Attention Mechanism\n",
        "\n",
        "Bab ini berfokus pada peningkatan model Seq2Seq menggunakan **Bahdanau Attention** (sering disebut *Additive Attention*). Mekanisme ini memungkinkan Decoder untuk fokus pada bagian tertentu dari kalimat sumber saat menghasilkan setiap kata terjemahan.\n",
        "\n",
        "---\n",
        "\n",
        "## 12.1 Mengapa Butuh Attention? (Model Improvement)\n",
        "Pada Seq2Seq standar, seluruh informasi kalimat sumber dipaksa masuk ke dalam satu *context vector* statis. Ini menciptakan *bottleneck* untuk kalimat yang panjang.\n",
        "* **Bahdanau Attention**: Menghasilkan representasi dinamis yang kaya dengan melihat kembali seluruh *hidden states* dari Encoder (masa lalu dan masa depan sekuens).\n",
        "* **Mekanisme**: Output dari layer Attention digabungkan (*concatenate*) dengan output GRU pada Decoder. Informasi gabungan ini kemudian diteruskan ke lapisan *dense* untuk memprediksi kata berikutnya.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 12.2 Implementasi Bahdanau Attention\n",
        "Karena mekanisme ini melibatkan logika kustom dalam perhitungan skor bobot, kita sering kali harus membangunnya secara manual.\n",
        "* **Keras Subclassing API**: Digunakan untuk membuat layer `Attention` kustom. Kita mendefinisikan bobot $W_1$, $W_2$, dan $V$ untuk menghitung skor keselarasan antara *hidden state* Decoder dan output Encoder.\n",
        "* **Tips Komputasi**: Menyamakan jumlah unit RNN pada Encoder dan Decoder sangat disarankan untuk menyederhanakan operasi perkalian matriks di dalam mekanisme attention.\n",
        "\n",
        "---\n",
        "\n",
        "## 12.3 Alur Membangun Model Akhir\n",
        "Membangun model Seq2Seq dengan Attention memerlukan penggabungan beberapa komponen fungsional:\n",
        "1.  **Vectorizer**: Menggunakan `get_vectorizer()` untuk standarisasi teks.\n",
        "2.  **Encoder**: Menggunakan `get_encoder()` untuk mengekstraksi fitur sekuensial.\n",
        "3.  **Final Model**: Menggunakan `get_final_seq2seq_model_with_attention()` untuk merakit Encoder, Decoder, dan Layer Attention menjadi satu kesatuan.\n",
        "4.  **Training**: Model dilatih menggunakan fungsi pembantu `train_model()` dengan menyuplai data yang sudah dibagi (`train_df`, `valid_df`, `test_df`) serta menentukan `batch_size`.\n",
        "\n",
        "---\n",
        "\n",
        "## 12.4 Visualizing the Attention (Heatmaps)\n",
        "Salah satu keunggulan luar biasa dari mekanisme Attention adalah **Interpretabilitas** (model bisa menjelaskan apa yang dia \"pikirkan\").\n",
        "* **Attention Visualizer**: Fungsi ini memuat model yang telah disimpan dan mengekstraksi bobot perhatian (*attention weights*) untuk setiap kata yang dihasilkan.\n",
        "* **Heatmap Matplotlib**: Menggunakan `matplotlib` untuk menampilkan matriks hubungan antara kata sumber (sumbu X) dan kata target (sumbu Y).\n",
        "* **Interpretasi Warna**: Warna yang lebih terang atau piksel yang lebih pekat menunjukkan bahwa model memberikan \"perhatian\" yang lebih besar pada kata sumber tersebut saat menerjemahkan kata target tertentu.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 12.5 Tip Implementasi di Colab\n",
        "Saat menggunakan Subclassing untuk Attention, pastikan kamu menghitung skor dengan benar sebelum menerapkan Softmax:\n",
        "\n",
        "```python\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query (decoder hidden state) shape: (batch_size, hidden size)\n",
        "        # values (encoder outputs) shape: (batch_size, max_len, hidden size)\n",
        "        \n",
        "        # Menghitung skor perhatian\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(tf.expand_dims(query, 1))))\n",
        "        \n",
        "        # attention_weights shape: (batch_size, max_len, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # Menghitung context_vector\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "WfxlAXot_Wef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NcHt6D5_RoU"
      },
      "outputs": [],
      "source": []
    }
  ]
}